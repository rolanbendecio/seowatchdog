{
  "name": "SEO Watchdog - Enhanced GSC BigQuery Analysis",
  "description": "Comprehensive SEO monitoring workflow with BigQuery integration, AI analysis, and automated reporting via Telegram",
  "version": "2.0",
  "author": "Enhanced by Claude AI",
  "nodes": [
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "field": "cronExpression",
              "value": "0 9 * * *"
            }
          ]
        }
      },
      "id": "scheduler",
      "name": "Daily Schedule",
      "type": "n8n-nodes-base.cron",
      "typeVersion": 1,
      "position": [240, 300],
      "notes": "üïò SCHEDULER: Triggers workflow daily at 9 AM\n\nüìã CONFIGURATION:\n‚Ä¢ Cron expression: 0 9 * * * (9 AM daily)\n‚Ä¢ Timezone: Uses server timezone\n‚Ä¢ Purpose: Automated daily SEO monitoring\n\nüîß CUSTOMIZATION:\n‚Ä¢ Change time: Modify cronExpression value\n‚Ä¢ Add days: Use specific weekdays (0=Sunday, 1=Monday, etc.)\n‚Ä¢ Examples:\n  - '0 9 * * 1-5' = Weekdays only\n  - '0 9,17 * * *' = 9 AM and 5 PM daily\n\n‚ö° NEXT STEP: Triggers both BigQuery data collection nodes simultaneously"
    },
    {
      "parameters": {
        "authentication": "serviceAccount",
        "projectId": "={{ $env.GCP_PROJECT_ID }}",
        "query": "-- üìä PERFORMANCE DATA QUERY\n-- This query analyzes Google Search Console performance data for the last 30 days\n-- It identifies pages with traffic issues, low CTR, and potential problems\n\nWITH recent_data AS (\n  -- üìÖ Collect recent 30 days of search performance data\n  SELECT \n    date,\n    page,\n    query,\n    clicks,\n    impressions,\n    SAFE_DIVIDE(clicks, impressions) * 100 as ctr,\n    position,\n    country,\n    device\n  FROM `{{ $env.GCP_PROJECT_ID }}.searchconsole.searchdata_site_impression`\n  WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)\n    AND date <= DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)\n),\n\npage_performance AS (\n  -- üìà Aggregate performance metrics by page\n  SELECT \n    page,\n    SUM(clicks) as total_clicks,\n    SUM(impressions) as total_impressions,\n    SAFE_DIVIDE(SUM(clicks), SUM(impressions)) * 100 as avg_ctr,\n    AVG(position) as avg_position,\n    COUNT(DISTINCT query) as keyword_count,\n    DATE_DIFF(CURRENT_DATE(), MAX(date), DAY) as days_since_last_data\n  FROM recent_data\n  GROUP BY page\n),\n\ntraffic_trends AS (\n  -- üìä Calculate weekly traffic trends\n  SELECT \n    page,\n    DATE_TRUNC(date, WEEK) as week,\n    SUM(clicks) as weekly_clicks,\n    SUM(impressions) as weekly_impressions\n  FROM recent_data\n  GROUP BY page, week\n),\n\nweekly_comparison AS (\n  -- üìâ Compare current week vs previous week performance\n  SELECT \n    page,\n    LAG(weekly_clicks, 1) OVER (PARTITION BY page ORDER BY week) as prev_week_clicks,\n    weekly_clicks as current_week_clicks,\n    SAFE_DIVIDE(weekly_clicks - LAG(weekly_clicks, 1) OVER (PARTITION BY page ORDER BY week), LAG(weekly_clicks, 1) OVER (PARTITION BY page ORDER BY week)) * 100 as click_change_percent\n  FROM traffic_trends\n  WHERE week = DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY), WEEK)\n)\n\n-- üéØ FINAL RESULTS: Top 100 pages with performance metrics and alerts\nSELECT \n  pp.page,\n  pp.total_clicks,\n  pp.total_impressions,\n  pp.avg_ctr,\n  pp.avg_position,\n  pp.keyword_count,\n  pp.days_since_last_data,\n  wc.click_change_percent,\n  -- üö® ALERT CLASSIFICATION SYSTEM\n  CASE \n    WHEN wc.click_change_percent < -20 THEN 'traffic_drop'      -- üìâ Significant traffic decrease\n    WHEN wc.click_change_percent > 20 THEN 'traffic_surge'      -- üìà Significant traffic increase\n    WHEN pp.avg_ctr < 2 AND pp.total_impressions > 1000 THEN 'low_ctr'  -- üéØ Low CTR optimization opportunity\n    WHEN pp.avg_position > 10 AND pp.total_impressions > 500 THEN 'low_position'  -- üîç Ranking improvement opportunity\n    WHEN pp.days_since_last_data > 7 THEN 'potential_deindex'  -- ‚ö†Ô∏è Potential indexing issue\n    ELSE 'normal'  -- ‚úÖ Normal performance\n  END as alert_type\nFROM page_performance pp\nLEFT JOIN weekly_comparison wc ON pp.page = wc.page\nORDER BY pp.total_clicks DESC\nLIMIT 100"
      },
      "id": "bigquery_gsc_data",
      "name": "BigQuery - GSC Performance Data",
      "type": "n8n-nodes-base.googleBigQuery",
      "typeVersion": 1,
      "position": [460, 250],
      "notes": "üìä BIGQUERY GSC DATA COLLECTION\n\nüéØ PURPOSE:\n‚Ä¢ Fetches Google Search Console performance data\n‚Ä¢ Analyzes traffic trends and identifies issues\n‚Ä¢ Categorizes pages by performance alerts\n\nüìã DATA COLLECTED:\n‚Ä¢ Page URLs and performance metrics\n‚Ä¢ Click/impression data for last 30 days\n‚Ä¢ CTR and position averages\n‚Ä¢ Week-over-week traffic changes\n‚Ä¢ Alert classifications\n\nüö® ALERT TYPES:\n‚Ä¢ traffic_drop: >20% traffic decrease\n‚Ä¢ traffic_surge: >20% traffic increase\n‚Ä¢ low_ctr: <2% CTR with >1000 impressions\n‚Ä¢ low_position: >10 position with >500 impressions\n‚Ä¢ potential_deindex: No data for >7 days\n\nüîß CONFIGURATION:\n‚Ä¢ Environment variable: GCP_PROJECT_ID\n‚Ä¢ Authentication: Service Account\n‚Ä¢ Query timeout: Default BigQuery limits\n‚Ä¢ Result limit: 100 top pages\n\n‚ö° NEXT STEP: Data flows to Combine Data node"
    },
    {
      "parameters": {
        "authentication": "serviceAccount",
        "projectId": "={{ $env.GCP_PROJECT_ID }}",
        "query": "-- üîç INDEXING STATUS QUERY\n-- This query monitors crawl status and indexing health\n-- It identifies newly indexed, deindexed, and problematic pages\n\nWITH indexed_pages AS (\n  -- üìã Recent crawl and indexing data\n  SELECT \n    page_url,\n    last_crawl_time,\n    coverage_state,\n    is_excluded,\n    DATE_DIFF(CURRENT_DATE(), DATE(last_crawl_time), DAY) as days_since_crawl\n  FROM `{{ $env.GCP_PROJECT_ID }}.searchconsole.crawl_stats`\n  WHERE DATE(last_crawl_time) >= DATE_SUB(CURRENT_DATE(), INTERVAL 14 DAY)\n),\n\nindexing_changes AS (\n  -- üîÑ Classify indexing status changes\n  SELECT \n    page_url,\n    coverage_state,\n    is_excluded,\n    days_since_crawl,\n    -- üìä INDEXING STATUS CLASSIFICATION\n    CASE \n      WHEN is_excluded = FALSE AND coverage_state = 'Valid' THEN 'newly_indexed'  -- ‚úÖ Successfully indexed\n      WHEN is_excluded = TRUE OR coverage_state != 'Valid' THEN 'deindexed'       -- ‚ùå Removed from index\n      WHEN days_since_crawl > 7 THEN 'crawl_issue'                               -- ‚ö†Ô∏è Crawling problems\n      ELSE 'normal'  -- ‚úÖ Normal indexing status\n    END as indexing_status\n  FROM indexed_pages\n)\n\n-- üéØ FINAL RESULTS: Pages with indexing issues only\nSELECT \n  page_url,\n  coverage_state,\n  is_excluded,\n  days_since_crawl,\n  indexing_status,\n  COUNT(*) OVER (PARTITION BY indexing_status) as status_count\nFROM indexing_changes\nWHERE indexing_status != 'normal'  -- Only show problematic pages\nORDER BY days_since_crawl DESC\nLIMIT 50"
      },
      "id": "bigquery_indexing_data",
      "name": "BigQuery - Indexing Status Data",
      "type": "n8n-nodes-base.googleBigQuery",
      "typeVersion": 1,
      "position": [460, 400],
      "notes": "üîç INDEXING STATUS MONITORING\n\nüéØ PURPOSE:\n‚Ä¢ Monitors Google Search Console crawl stats\n‚Ä¢ Identifies indexing issues and changes\n‚Ä¢ Tracks newly indexed and deindexed pages\n\nüìã DATA COLLECTED:\n‚Ä¢ Page URLs and crawl timestamps\n‚Ä¢ Coverage states and exclusion status\n‚Ä¢ Days since last crawl\n‚Ä¢ Indexing status classifications\n\nüö® STATUS TYPES:\n‚Ä¢ newly_indexed: Recently added to index\n‚Ä¢ deindexed: Removed from search index\n‚Ä¢ crawl_issue: Not crawled for >7 days\n‚Ä¢ normal: No issues (filtered out)\n\nüîß CONFIGURATION:\n‚Ä¢ Environment variable: GCP_PROJECT_ID\n‚Ä¢ Authentication: Service Account\n‚Ä¢ Time range: Last 14 days of crawl data\n‚Ä¢ Result limit: 50 problematic pages\n\n‚ö° NEXT STEP: Data flows to Combine Data node"
    },
    {
      "parameters": {
        "jsCode": "// üîÑ DATA COMBINATION AND PREPARATION\n// This node combines performance and indexing data for AI analysis\n\n// üìä Extract data from both BigQuery sources\nconst gscData = $input.first().json;  // Performance data from first BigQuery node\nconst indexingData = $input.last().json;  // Indexing data from second BigQuery node\n\n// üìã Create combined analysis payload\nconst combineData = {\n  // üéØ PERFORMANCE METRICS\n  performance_data: gscData,\n  \n  // üîç INDEXING STATUS\n  indexing_data: indexingData,\n  \n  // üìÖ METADATA\n  analysis_date: new Date().toISOString().split('T')[0],\n  total_pages_analyzed: gscData.length,\n  indexing_issues: indexingData.length,\n  \n  // üìä SUMMARY STATISTICS\n  summary: {\n    total_clicks: gscData.reduce((sum, page) => sum + (page.total_clicks || 0), 0),\n    total_impressions: gscData.reduce((sum, page) => sum + (page.total_impressions || 0), 0),\n    avg_ctr: gscData.reduce((sum, page) => sum + (page.avg_ctr || 0), 0) / gscData.length,\n    alert_counts: {\n      traffic_drops: gscData.filter(p => p.alert_type === 'traffic_drop').length,\n      traffic_surges: gscData.filter(p => p.alert_type === 'traffic_surge').length,\n      low_ctr: gscData.filter(p => p.alert_type === 'low_ctr').length,\n      low_position: gscData.filter(p => p.alert_type === 'low_position').length,\n      potential_deindex: gscData.filter(p => p.alert_type === 'potential_deindex').length\n    },\n    indexing_status_counts: {\n      newly_indexed: indexingData.filter(p => p.indexing_status === 'newly_indexed').length,\n      deindexed: indexingData.filter(p => p.indexing_status === 'deindexed').length,\n      crawl_issues: indexingData.filter(p => p.indexing_status === 'crawl_issue').length\n    }\n  }\n};\n\n// üéØ Return combined data for AI analysis\nreturn { json: combineData };"
      },
      "id": "combine_data",
      "name": "Combine & Prepare Data",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [680, 325],
      "notes": "üîÑ DATA COMBINATION & PREPARATION\n\nüéØ PURPOSE:\n‚Ä¢ Combines BigQuery performance and indexing data\n‚Ä¢ Calculates summary statistics\n‚Ä¢ Prepares structured data for AI analysis\n\nüìä DATA PROCESSING:\n‚Ä¢ Merges performance metrics with indexing status\n‚Ä¢ Calculates totals and averages\n‚Ä¢ Counts alerts by type\n‚Ä¢ Generates analysis metadata\n\nüìã OUTPUT STRUCTURE:\n‚Ä¢ performance_data: Page performance metrics\n‚Ä¢ indexing_data: Crawl and indexing status\n‚Ä¢ analysis_date: Current date\n‚Ä¢ total_pages_analyzed: Count of analyzed pages\n‚Ä¢ indexing_issues: Count of indexing problems\n‚Ä¢ summary: Aggregated statistics and alert counts\n\nüîß CUSTOMIZATION:\n‚Ä¢ Modify summary calculations\n‚Ä¢ Add new alert type counting\n‚Ä¢ Include additional metadata\n\n‚ö° NEXT STEP: Structured data sent to Claude AI"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.anthropic.com/v1/messages",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "headers": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "body": {
          "parameters": [
            {
              "name": "model",
              "value": "claude-3-5-sonnet-20241022"
            },
            {
              "name": "max_tokens",
              "value": 3000
            },
            {
              "name": "messages",
              "value": "=[{\n  \"role\": \"user\",\n  \"content\": `You are an expert SEO analyst providing actionable insights for website optimization. Analyze the following Google Search Console data and provide strategic recommendations.\n\nüîç ANALYSIS CONTEXT:\n‚Ä¢ Analysis Date: ${$json.analysis_date}\n‚Ä¢ Pages Analyzed: ${$json.total_pages_analyzed}\n‚Ä¢ Indexing Issues: ${$json.indexing_issues}\n‚Ä¢ Total Site Clicks: ${$json.summary.total_clicks.toLocaleString()}\n‚Ä¢ Total Site Impressions: ${$json.summary.total_impressions.toLocaleString()}\n‚Ä¢ Average Site CTR: ${$json.summary.avg_ctr.toFixed(2)}%\n\nüìä PERFORMANCE DATA:\n${JSON.stringify($json.performance_data, null, 2)}\n\nüîç INDEXING DATA:\n${JSON.stringify($json.indexing_data, null, 2)}\n\nüìã ALERT SUMMARY:\n‚Ä¢ Traffic Drops: ${$json.summary.alert_counts.traffic_drops}\n‚Ä¢ Traffic Surges: ${$json.summary.alert_counts.traffic_surges}\n‚Ä¢ Low CTR Opportunities: ${$json.summary.alert_counts.low_ctr}\n‚Ä¢ Low Position Opportunities: ${$json.summary.alert_counts.low_position}\n‚Ä¢ Potential Deindexing: ${$json.summary.alert_counts.potential_deindex}\n\nüîç INDEXING STATUS:\n‚Ä¢ Newly Indexed: ${$json.summary.indexing_status_counts.newly_indexed}\n‚Ä¢ Deindexed: ${$json.summary.indexing_status_counts.deindexed}\n‚Ä¢ Crawl Issues: ${$json.summary.indexing_status_counts.crawl_issues}\n\nProvide analysis in this EXACT format:\n\nüîç **SEO WATCHDOG REPORT - ${$json.analysis_date}**\n\n**üìä EXECUTIVE SUMMARY:**\n‚Ä¢ Total Pages Analyzed: ${$json.total_pages_analyzed}\n‚Ä¢ Indexing Issues Found: ${$json.indexing_issues}\n‚Ä¢ Site Performance: [Brief overall assessment]\n‚Ä¢ Key Concern: [Most critical issue if any]\n\n**üö® CRITICAL ALERTS:**\n[List urgent issues requiring immediate attention - traffic drops >20%, major indexing problems, etc.]\n\n**üìâ TRAFFIC PERFORMANCE:**\n‚Ä¢ Traffic Drops: [Pages with significant decreases]\n‚Ä¢ Traffic Surges: [Pages with significant increases]\n‚Ä¢ Overall Trend: [Week-over-week analysis]\n\n**üéØ OPTIMIZATION OPPORTUNITIES:**\n‚Ä¢ Low CTR Pages: [High impression, low CTR pages with specific recommendations]\n‚Ä¢ Position Improvements: [Pages ranking 10+ that could move higher]\n‚Ä¢ Content Gaps: [Keywords/pages that need attention]\n\n**üîç INDEXING & TECHNICAL ISSUES:**\n‚Ä¢ Newly Indexed: [Recently added pages]\n‚Ä¢ Deindexed Pages: [Pages removed from index]\n‚Ä¢ Crawl Problems: [Technical issues affecting crawling]\n\n**‚úÖ ACTIONABLE RECOMMENDATIONS:**\n1. [Specific action item with expected impact]\n2. [Another specific action item]\n3. [Third specific action item]\n\n**üöÄ PRIORITY ACTIONS (Next 7 Days):**\n‚Ä¢ [Immediate action 1]\n‚Ä¢ [Immediate action 2]\n‚Ä¢ [Immediate action 3]\n\n**üìà GROWTH OPPORTUNITIES:**\n‚Ä¢ [High-potential optimization with estimated impact]\n‚Ä¢ [Content opportunity with traffic potential]\n‚Ä¢ [Technical improvement with SEO benefits]\n\nKeep recommendations specific, actionable, and prioritized. Focus on pages with highest traffic potential and clearest optimization paths.`\n}]"
            }
          ]
        }
      },
      "id": "claude_analysis",
      "name": "Claude AI SEO Analysis",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [900, 325],
      "notes": "ü§ñ CLAUDE AI SEO ANALYSIS\n\nüéØ PURPOSE:\n‚Ä¢ Analyzes combined SEO data using Claude AI\n‚Ä¢ Provides expert insights and recommendations\n‚Ä¢ Identifies optimization opportunities\n‚Ä¢ Generates actionable SEO strategies\n\nüß† AI CAPABILITIES:\n‚Ä¢ Expert SEO knowledge and analysis\n‚Ä¢ Pattern recognition in performance data\n‚Ä¢ Prioritized recommendation generation\n‚Ä¢ Strategic insights for growth\n\nüìä ANALYSIS INCLUDES:\n‚Ä¢ Executive summary with key metrics\n‚Ä¢ Critical alerts requiring immediate attention\n‚Ä¢ Traffic performance trends and changes\n‚Ä¢ Optimization opportunities (CTR, position)\n‚Ä¢ Indexing and technical issue identification\n‚Ä¢ Actionable recommendations with priorities\n‚Ä¢ Growth opportunities and potential impact\n\nüîß CONFIGURATION:\n‚Ä¢ Model: claude-3-5-sonnet-20241022\n‚Ä¢ Max tokens: 3000 (comprehensive analysis)\n‚Ä¢ Authentication: API key via environment variable\n‚Ä¢ Response format: Structured markdown report\n\nüìã REPORT SECTIONS:\n‚Ä¢ Executive Summary\n‚Ä¢ Critical Alerts\n‚Ä¢ Traffic Performance\n‚Ä¢ Optimization Opportunities\n‚Ä¢ Indexing & Technical Issues\n‚Ä¢ Actionable Recommendations\n‚Ä¢ Priority Actions (Next 7 Days)\n‚Ä¢ Growth Opportunities\n\n‚ö° NEXT STEP: AI-generated report sent to formatting"
    },
    {
      "parameters": {
        "jsCode": "// üì± TELEGRAM REPORT FORMATTING\n// This node formats the AI analysis for Telegram delivery\n\n// üìä Extract Claude's analysis response\nconst response = $json.content[0].text;\n\n// üéØ Format for Telegram with enhanced structure\nconst formatForTelegram = {\n  // üìã MAIN REPORT CONTENT\n  report: response,\n  \n  // üìÖ METADATA\n  timestamp: new Date().toISOString(),\n  date_readable: new Date().toLocaleDateString('en-US', {\n    weekday: 'long',\n    year: 'numeric',\n    month: 'long',\n    day: 'numeric'\n  }),\n  \n  // üì± TELEGRAM CONFIGURATION\n  chat_id: $env.TELEGRAM_CHAT_ID,\n  parse_mode: 'Markdown',\n  \n  // üîß TELEGRAM FORMATTING OPTIONS\n  disable_web_page_preview: true,  // Prevent URL previews\n  disable_notification: false,     // Allow notifications\n  \n  // üìä REPORT STATISTICS\n  report_stats: {\n    character_count: response.length,\n    word_count: response.split(' ').length,\n    section_count: (response.match(/\\*\\*/g) || []).length,\n    emoji_count: (response.match(/[\\u{1F600}-\\u{1F64F}]|[\\u{1F300}-\\u{1F5FF}]|[\\u{1F680}-\\u{1F6FF}]|[\\u{1F1E0}-\\u{1F1FF}]|[\\u{2600}-\\u{26FF}]|[\\u{2700}-\\u{27BF}]/gu) || []).length\n  }\n};\n\n// üéØ Return formatted data for Telegram\nreturn { json: formatForTelegram };"
      },
      "id": "format_report",
      "name": "Format Telegram Report",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1120, 325],
      "notes": "üì± TELEGRAM REPORT FORMATTING\n\nüéØ PURPOSE:\n‚Ä¢ Formats AI analysis for Telegram delivery\n‚Ä¢ Adds metadata and delivery configuration\n‚Ä¢ Prepares message parameters\n‚Ä¢ Calculates report statistics\n\nüìã FORMATTING FEATURES:\n‚Ä¢ Markdown formatting support\n‚Ä¢ Emoji preservation\n‚Ä¢ URL preview control\n‚Ä¢ Notification settings\n‚Ä¢ Character and word counting\n\nüîß TELEGRAM CONFIGURATION:\n‚Ä¢ Chat ID: From environment variable\n‚Ä¢ Parse mode: Markdown for formatting\n‚Ä¢ Web preview: Disabled for cleaner look\n‚Ä¢ Notifications: Enabled for alerts\n\nüìä REPORT METADATA:\n‚Ä¢ Timestamp: ISO format\n‚Ä¢ Readable date: Localized format\n‚Ä¢ Character count: For length tracking\n‚Ä¢ Word count: Content analysis\n‚Ä¢ Section count: Structure analysis\n‚Ä¢ Emoji count: Visual element tracking\n\nüé® CUSTOMIZATION:\n‚Ä¢ Modify parse_mode for different formatting\n‚Ä¢ Add custom message threading\n‚Ä¢ Include report summary statistics\n‚Ä¢ Add custom formatting rules\n\n‚ö° NEXT STEP: Formatted report sent to Telegram"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "=https://api.telegram.org/bot{{ $env.TELEGRAM_BOT_TOKEN }}/sendMessage",
        "headers": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "body": {
          "parameters": [
            {
              "name": "chat_id",
              "value": "={{ $json.chat_id }}"
            },
            {
              "name": "text",
              "value": "={{ $json.report }}"
            },
            {
              "name": "parse_mode",
              "value": "={{ $json.parse_mode }}"
            },
            {
              "name": "disable_web_page_preview",
              "value": "={{ $json.disable_web_page_preview }}"
            },
            {
              "name": "disable_notification",
              "value": "={{ $json.disable_notification }}"
            }
          ]
        }
      },
      "id": "send_telegram",
      "name": "Send to Telegram",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [1340, 325],
      "notes": "üì± TELEGRAM MESSAGE DELIVERY\n\nüéØ PURPOSE:\n‚Ä¢ Sends formatted SEO report to Telegram\n‚Ä¢ Delivers daily insights to stakeholders\n‚Ä¢ Provides mobile-friendly notifications\n‚Ä¢ Enables quick access to SEO alerts\n\nüìã TELEGRAM FEATURES:\n‚Ä¢ Markdown formatting support\n‚Ä¢ Emoji and formatting preservation\n‚Ä¢ URL preview control\n‚Ä¢ Notification management\n‚Ä¢ Mobile-optimized display\n\nüîß CONFIGURATION:\n‚Ä¢ Bot token: From environment variable\n‚Ä¢ Chat ID: Target chat/channel\n‚Ä¢ Message formatting: Markdown\n‚Ä¢ Web previews: Disabled\n‚Ä¢ Notifications: Enabled\n\nüìä DELIVERY PARAMETERS:\n‚Ä¢ Method: POST to Telegram Bot API\n‚Ä¢ Endpoint: /sendMessage\n‚Ä¢ Content-Type: application/json\n‚Ä¢ Authentication: Bot token in URL\n\nüé® MESSAGE FEATURES:\n‚Ä¢ Rich formatting with emojis\n‚Ä¢ Structured sections\n‚Ä¢ Clickable elements\n‚Ä¢ Mobile-responsive layout\n\n‚ö†Ô∏è ERROR HANDLING:\n‚Ä¢ API rate limits respected\n‚Ä¢ Message length limits considered\n‚Ä¢ Network timeout handling\n‚Ä¢ Delivery confirmation tracking\n\n‚ö° NEXT STEP: Delivery confirmation logged"
    },
    {
      "parameters": {
        "jsCode": "// üìä DELIVERY RESULT LOGGING\n// This node processes and logs the Telegram delivery result\n\n// üìã Extract delivery response\nconst success = $json.ok;\nconst messageId = $json.message_id;\nconst errorDescription = $json.description;\n\n// üìÖ Generate timestamp\nconst timestamp = new Date().toISOString();\nconst readableTime = new Date().toLocaleString('en-US', {\n  timeZone: 'UTC',\n  weekday: 'long',\n  year: 'numeric',\n  month: 'long',\n  day: 'numeric',\n  hour: '2-digit',\n  minute: '2-digit',\n  second: '2-digit'\n});\n\n// üìä Log delivery status\nif (success) {\n  console.log(`‚úÖ SEO REPORT DELIVERED SUCCESSFULLY`);\n  console.log(`üì± Telegram Message ID: ${messageId}`);\n  console.log(`‚è∞ Delivery Time: ${readableTime}`);\n  console.log(`üéØ Report Status: DELIVERED`);\n} else {\n  console.log(`‚ùå SEO REPORT DELIVERY FAILED`);\n  console.log(`‚ö†Ô∏è Error: ${errorDescription}`);\n  console.log(`‚è∞ Failed At: ${readableTime}`);\n  console.log(`üéØ Report Status: FAILED`);\n}\n\n// üîç Additional logging for monitoring\nconsole.log(`üìä Workflow Execution: SEO Watchdog`);\nconsole.log(`üîÑ Process: Daily Analysis Complete`);\nconsole.log(`üìà Next Run: Tomorrow at 9 AM`);\n\n// üìã Return structured result\nreturn { \n  json: {\n    // üìä DELIVERY STATUS\n    success: success,\n    message_id: messageId,\n    error_description: errorDescription,\n    \n    // üìÖ TIMING INFORMATION\n    timestamp: timestamp,\n    readable_time: readableTime,\n    \n    // üìã WORKFLOW STATUS\n    workflow_status: success ? 'COMPLETED' : 'FAILED',\n    next_execution: 'Tomorrow at 9 AM',\n    \n    // üéØ SUMMARY\n    summary: {\n      process: 'SEO Watchdog Daily Analysis',\n      status: success ? 'SUCCESS' : 'FAILED',\n      delivery_method: 'Telegram',\n      completion_time: readableTime\n    }\n  }\n};"
      },
      "id": "log_result",
      "name": "Log Delivery Result",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1560, 325],
      "notes": "üìä DELIVERY RESULT LOGGING\n\nüéØ PURPOSE:\n‚Ä¢ Logs Telegram delivery success/failure\n‚Ä¢ Tracks message delivery status\n‚Ä¢ Provides workflow completion confirmation\n‚Ä¢ Enables monitoring and troubleshooting\n\nüìã LOGGING FEATURES:\n‚Ä¢ Success/failure status tracking\n‚Ä¢ Telegram message ID capture\n‚Ä¢ Error description logging\n‚Ä¢ Timestamp recording\n‚Ä¢ Workflow status summary\n\nüîç CONSOLE OUTPUT:\n‚Ä¢ Delivery confirmation with emojis\n‚Ä¢ Message ID for reference\n‚Ä¢ Readable timestamp\n‚Ä¢ Next execution schedule\n‚Ä¢ Error details if applicable\n\nüìä RETURN DATA:\n‚Ä¢ Structured delivery result\n‚Ä¢ Timing information\n‚Ä¢ Workflow status\n‚Ä¢ Summary statistics\n\nüéØ MONITORING BENEFITS:\n‚Ä¢ Track delivery reliability\n‚Ä¢ Identify failure patterns\n‚Ä¢ Monitor workflow health\n‚Ä¢ Debug delivery issues\n\n‚ö° WORKFLOW COMPLETE: SEO analysis cycle finished"
    }
  ],
  "connections": {
    "Daily Schedule": {
      "main": [
        [
          {
            "node": "BigQuery - GSC Performance Data",
            "type": "main",
            "index": 0
          },
          {
            "node": "BigQuery - Indexing Status Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "BigQuery - GSC Performance Data": {
      "main": [
        [
          {
            "node": "Combine & Prepare Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "BigQuery - Indexing Status Data": {
      "main": [
        [
          {
            "node": "Combine & Prepare Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Combine & Prepare Data": {
      "main": [
        [
          {
            "node": "Claude AI SEO Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Claude AI SEO Analysis": {
      "main": [
        [
          {
            "node": "Format Telegram Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Telegram Report": {
      "main": [
        [
          {
            "node": "Send to Telegram",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Send to Telegram": {
      "main": [
        [
          {
            "node": "Log Delivery Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1",
    "saveManualExecutions": true,
    "callerPolicy": "workflowsFromSameOwner",
    "errorWorkflow": {
      "enabled": false
    }
  },
  "staticData": null,
  "tags": [
    {
      "createdAt": "2024-12-07T00:00:00.000Z",
      "updatedAt": "2024-12-07T00:00:00.000Z",
      "id": "seo",
      "name": "SEO"
    },
    {
      "createdAt": "2024-12-07T00:00:00.000Z",
      "updatedAt": "2024-12-07T00:00:00.000Z",
      "id": "enhanced",
      "name": "Enhanced"
    },
    {
      "createdAt": "2024-12-07T00:00:00.000Z",
      "updatedAt": "2024-12-07T00:00:00.000Z",
      "id": "bigquery",
      "name": "BigQuery"
    },
    {
      "createdAt": "2024-12-07T00:00:00.000Z",
      "updatedAt": "2024-12-07T00:00:00.000Z",
      "id": "ai-analysis",
      "name": "AI Analysis"
    }
  ],
  "triggerCount": 1,
  "updatedAt": "2024-12-07T00:00:00.000Z",
  "versionId": "2.0-enhanced"
}